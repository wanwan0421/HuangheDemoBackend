"""
æ¨¡å‹éœ€æ±‚è¾“å…¥æ•°æ®æ‰«æ - LangGraph å·¥ä½œæµ
æ‰§è¡Œæµç¨‹ï¼šè§£æ MDL â†’ æå–æ¨¡å‹è¾“å…¥/è¾“å‡º/å‚æ•°éœ€æ±‚
"""

import json
from typing import List
from langgraph.graph import START, END, StateGraph, DEFAULT_CONDITIONAL
from langgraph.prebuilt import ToolNode
from langchain.messages import HumanMessage, ToolMessage, AIMessage
from tools import (
    ModelRequirementState,
    model_with_tools,
    TOOLS_BY_NAME,
    tool_parse_mdl,
)


# ============================================================================
# èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def llm_node(state: ModelRequirementState):
    """
    LLM èŠ‚ç‚¹ - å†³å®šä¸‹ä¸€æ­¥æ“ä½œï¼ˆä»…è¿›è¡Œ MDL æ‰«æä¸éœ€æ±‚æå–ï¼‰
    """
    messages = state["messages"]

    # å¦‚æœæ˜¯åˆå§‹è¯·æ±‚ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
    if not messages or (len(messages) == 1 and isinstance(messages[0], HumanMessage)):
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ¨¡å‹éœ€æ±‚æ‰«æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. è§£ææ¨¡å‹çš„ MDL æ–‡ä»¶ï¼Œæå–æ¨¡å‹çš„è¾“å…¥éœ€æ±‚ã€è¾“å‡ºè§„èŒƒä¸å‚æ•°è®¾ç½®
    2. ä»…è¿›è¡Œéœ€æ±‚æ‰«æï¼Œä¸è¿›è¡Œä»»ä½•ç”¨æˆ·æ•°æ®çš„éªŒè¯æˆ–æ¯”å¯¹

    æ‰§è¡Œæ­¥éª¤ï¼š
    1. ä½¿ç”¨ tool_parse_mdl è§£æ MDL æ•°æ®å¹¶è¿”å›ç»“æ„åŒ–çš„æ¨¡å‹éœ€æ±‚ JSON

    è¯·ä¸¥æ ¼éµå¾ªä»…æ‰«æ/è§£æçš„èŒƒå›´ï¼Œä¸è¿›è¡Œæ•°æ®éªŒè¯ã€‚"""

        messages = [HumanMessage(content=system_prompt)] + messages

    # è°ƒç”¨ LLM
    response = model_with_tools.invoke(messages)

    return {
        "messages": [response],
        "mdl_requirements": state.get("mdl_requirements", {}),
        "status": state.get("status", "processing")
    }


def tool_node(state: ModelRequirementState):
    """
    å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ - æ‰§è¡Œ LLM æŒ‡å®šçš„å·¥å…·ï¼ˆä»…æ”¯æŒ MDL è§£æï¼‰
    """
    messages = state["messages"]
    last_message = messages[-1]

    # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_results = []

        for tool_call in last_message.tool_calls:
            tool_name = tool_call["name"]
            tool_input = tool_call["args"]

            print(f"\nğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
            print(f"   è¾“å…¥: {json.dumps(tool_input, ensure_ascii=False, indent=2)[:200]}...")

            # æ‰§è¡Œå·¥å…·
            if tool_name in TOOLS_BY_NAME:
                try:
                    tool = TOOLS_BY_NAME[tool_name]
                    result = tool.invoke(tool_input)

                    print(f"   ç»“æœ: æˆåŠŸ")

                    tool_results.append(
                        ToolMessage(
                            content=json.dumps(result, ensure_ascii=False),
                            tool_use_id=tool_call["id"],
                            name=tool_name
                        )
                    )

                    # æ›´æ–°çŠ¶æ€
                    if tool_name == "tool_parse_mdl":
                        state["mdl_requirements"] = result

                except Exception as e:
                    tool_results.append(
                        ToolMessage(
                            content=f"Error: {str(e)}",
                            tool_use_id=tool_call["id"],
                            name=tool_name
                        )
                    )

        return {
            "messages": tool_results,
            "mdl_requirements": state.get("mdl_requirements", {}),
            "status": state.get("status", "processing")
        }

    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›åŸçŠ¶æ€
    return state


def should_continue(state: ModelRequirementState) -> str:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰§è¡Œå·¥å…·æˆ–ç»“æŸæµç¨‹
    """
    messages = state["messages"]

    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
    last_message = messages[-1]

    # å¦‚æœæ˜¯ ToolMessageï¼Œç»§ç»­è°ƒç”¨ LLM
    if isinstance(last_message, ToolMessage):
        return "llm"

    # å¦‚æœæ˜¯ AIMessage ä¸”æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œå·¥å…·
    if isinstance(last_message, AIMessage):
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"

    # å¦åˆ™ç»“æŸæµç¨‹
    return "end"


# ============================================================================
# æ„å»º LangGraph
# ============================================================================

def build_model_requirement_graph():
    """
    æ„å»ºæ¨¡å‹éœ€æ±‚æ‰«æå›¾ï¼ˆä»…è§£æ MDL å¹¶æå–éœ€æ±‚ï¼‰
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    graph_builder = StateGraph(ModelRequirementState)

    # æ·»åŠ èŠ‚ç‚¹
    graph_builder.add_node("llm", llm_node)
    graph_builder.add_node("tools", tool_node)

    # æ·»åŠ è¾¹
    graph_builder.add_edge(START, "llm")

    # æ¡ä»¶è·¯ç”±
    graph_builder.add_conditional_edges(
        "llm",
        should_continue,
        {
            "tools": "tools",
            "llm": "llm",
            "end": END
        }
    )

    graph_builder.add_conditional_edges(
        "tools",
        should_continue,
        {
            "llm": "llm",
            "tools": "tools",
            "end": END
        }
    )

    # ç¼–è¯‘å›¾
    return graph_builder.compile()


# åˆ›å»ºå…¨å±€å›¾å®ä¾‹
model_requirement_graph = build_model_requirement_graph()
