from fastapi import FastAPI, WebSocket, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from agents.nodes import agent
from langchain.messages import HumanMessage
from typing import Any, Dict, List, Optional
import uuid
import json
import asyncio

app = FastAPI()
# å…è®¸ä»»ä½•æ¥æºçš„è·¨åŸŸè¯·æ±‚
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

def serialize_message(msg: Any) -> Dict[str, Any]:
    """
    æŠŠlangchain/message-likeå¯¹è±¡åºåˆ—åŒ–ä¸ºå‰ç«¯å¯ä½¿ç”¨çš„ç®€å•çš„dict
    Args:
        msg (Any): langchainæ¶ˆæ¯å¯¹è±¡
    Returns:
        Dict[str, Any]: åºåˆ—åŒ–åçš„æ¶ˆæ¯å­—å…¸
    """

    out: Dict[str, Any] = {}
    out["content"] = getattr(msg, "content", None)

    # æ¨æ–­è§’è‰²
    cls_name = msg.__class__.__name__ if hasattr(msg, "__class__") else None
    if cls_name:
        if "Human" in cls_name or "User" in cls_name:
            out["role"] = "user"
        elif "System" in cls_name:
            out["role"] = "system"
        elif "Tool" in cls_name:
            out["role"] = "tool"
        else:
            out["role"] = "assistant"
    
    # tool_callså¦‚æœå­˜åœ¨
    if hasattr(msg, "tool_calls"):
        try:
            out["tool_calls"] = list(getattr(msg, "tool_calls") or [])
        except Exception:
            out["tool_calls"] = getattr(msg, "tool_calls")
    
    # tool_call_idï¼ˆToolMessageï¼‰
    if hasattr(msg, "tool_call_id"):
        out["tool_call_id"] = getattr(msg, "tool_call_id")
    return out

def serialize_messages(msgs: List[Any]) -> List[Dict[str, Any]]:
    return [serialize_message(m) for m in msgs]

def map_agent_event(event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    å°†LangGraphåŸå§‹äº‹ä»¶æ˜ å°„ä¸ºâ€œå¯è§£é‡Šäº‹ä»¶â€
    """
    etype = event.get("event")
    print(f"Mapping event: {etype}")

    # Agent å¼€å§‹
    if etype == "on_chain_start":
        return {
            "type": "status",
            "message": "ğŸ§  Agent æ­£åœ¨åˆ†æé—®é¢˜"
        }

    # LLM å¼€å§‹ç”Ÿæˆ
    if etype == "on_chat_model_start":
        return {
            "type": "status",
            "message": "âœï¸ Agent æ­£åœ¨ç”Ÿæˆå›ç­”"
        }

    # LLM token æµ
    if etype == "on_chat_model_stream":
        chunk = event.get("data", {}).get("chunk")
        content = getattr(chunk, "content", "")
        if content:
            return {
                "type": "token",
                "message": content
            }

    # Tool è°ƒç”¨å¼€å§‹
    if etype == "on_tool_start":
        name = event.get('name')
        friendly_names = {
            "search_relevant_indices": "æ­£åœ¨æ£€ç´¢åœ°ç†æŒ‡æ ‡åº“...",
            "search_relevant_models": "æ­£åœ¨åŒ¹é…æœ€ä¼˜åœ°ç†æ¨¡å‹...",
            "get_model_details": "æ­£åœ¨è¯»å–æ¨¡å‹å·¥ä½œæµè¯¦æƒ…..."
        }
        msg = friendly_names.get(name, f"æ­£åœ¨æ‰§è¡Œå·¥å…·: {name}")
        return {
            "type": "tool",
            "message": f"ğŸ”§ {msg}",
            "data": event.get("data")
        }

    # Tool è°ƒç”¨ç»“æŸ
    if etype == "on_tool_end":
        return {
            "type": "tool",
            "message": "ğŸ“Š å·¥å…·å·²è¿”å›ç»“æœ",
            "data": event.get("data")
        }

    # Agent å®Œæˆ
    if etype == "on_chain_end":
        return {
            "type": "final",
            "message": "âœ… Agent å·²å¾—å‡ºç»“è®º"
        }

    return None

@app.post("/api/agent/run")
async def run_agent(body: Dict[str, Any]):
    query = body.get("query", "")
    if not query:
        raise HTTPException(status_code=400, detail="query is required")

    session_id = body.get("sessionId") or str(uuid.uuid4())
    
    # æ„é€ åˆå§‹çŠ¶æ€
    init_input = {"messages": [HumanMessage(content=query)]}

    try:
        # ç›´æ¥è¿è¡Œ Graphï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç† llm_node -> tool_node -> should_continue çš„å¾ªç¯
        # å¦‚æœä½ çš„ llm_node å®šä¹‰äº† llm_callsï¼Œå®ƒä¹Ÿä¼šåœ¨ç»“æœä¸­è¿”å›
        final_state = await agent.ainvoke(init_input)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Graph è¿è¡Œå¤±è´¥: {str(e)}")

    return {
        "status": "ok", 
        "sessionId": session_id, 
        "messages": serialize_messages(final_state.get("messages", [])), 
        "llm_calls": final_state.get("llm_calls", 0)
    }

@app.get("/api/agent/stream")
async def stream_agent(query: str):
    if not query:
        raise HTTPException(status_code=400, detail="query is required")

    async def event_generator():
        init_input = {
            "messages": [HumanMessage(content=query)]
        }

        try:
            async for event in agent.astream_events(
                init_input,
                version="v1"
            ):
                mapped = map_agent_event(event)
                if mapped:
                    yield f"data: {json.dumps(mapped, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0)
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )